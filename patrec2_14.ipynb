{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d3de59d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total wavs: 3000. Fs = 8000 Hz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting mfcc features...: 100%|████████████████████████████████████████████████| 3000/3000 [00:08<00:00, 336.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction completed with 6 mfccs per frame\n",
      "Normalization will be performed using mean: [-517.71365072   62.17300245   19.0018117     9.6444396   -19.211481\n",
      "  -10.88157066]\n",
      "Normalization will be performed using std: [152.11675796  52.36822505  36.82452714  29.71210521  24.82190463\n",
      "  23.43786656]\n",
      "Splitting in train test split using the default dataset split\n",
      "6\n",
      "540\n",
      "10\n",
      "2700\n"
     ]
    }
   ],
   "source": [
    "#9\n",
    "\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def parse_free_digits(directory):\n",
    "    # Parse relevant dataset info\n",
    "    files = glob(os.path.join(directory, \"*.wav\"))\n",
    "    \n",
    "    fnames = [f.split(\"\\\\\")[1].split(\".\")[0].split(\"_\") for f in files]\n",
    "    \n",
    "    \n",
    "    ids = [f[2] for f in fnames]\n",
    "    y = [int(f[0]) for f in fnames]\n",
    "    speakers = [f[1] for f in fnames]\n",
    "    _, Fs = librosa.core.load(files[0], sr=None)\n",
    "\n",
    "    def read_wav(f):\n",
    "        wav, _ = librosa.core.load(f, sr=None)\n",
    "\n",
    "        return wav\n",
    "\n",
    "    # Read all wavs\n",
    "    wavs = [read_wav(f) for f in files]\n",
    "\n",
    "    # Print dataset info\n",
    "    print(\"Total wavs: {}. Fs = {} Hz\".format(len(wavs), Fs))\n",
    "    #print(speakers)\n",
    "    return wavs, Fs, ids, y, speakers\n",
    "\n",
    "\n",
    "def extract_features(wavs, n_mfcc=6, Fs=8000):\n",
    "    # Extract MFCCs for all wavs\n",
    "    #print(len(wavs))\n",
    "    window = 30 * Fs // 1000\n",
    "    step = window // 2\n",
    "    frames = [\n",
    "        librosa.feature.mfcc(\n",
    "            wav, Fs, n_fft=window, hop_length=window - step, n_mfcc=n_mfcc\n",
    "        ).T\n",
    "\n",
    "        for wav in tqdm(wavs, desc=\"Extracting mfcc features...\")\n",
    "    ]\n",
    "\n",
    "    print(\"Feature extraction completed with {} mfccs per frame\".format(n_mfcc))\n",
    "     \n",
    "    return frames\n",
    "\n",
    "\n",
    "def split_free_digits(frames, ids, speakers, labels):\n",
    "    print(\"Splitting in train test split using the default dataset split\")\n",
    "    # Split to train-test\n",
    "    X_train, y_train, spk_train = [], [], []\n",
    "    X_test, y_test, spk_test = [], [], []\n",
    "    test_indices = [\"0\", \"1\", \"2\", \"3\", \"4\"]\n",
    "    \n",
    "    for idx, frame, label, spk in zip(ids, frames, labels, speakers):\n",
    "        if str(idx) in test_indices:\n",
    "            X_test.append(frame)\n",
    "            y_test.append(label)\n",
    "            spk_test.append(spk)\n",
    "        else:\n",
    "            X_train.append(frame)\n",
    "            y_train.append(label)\n",
    "            spk_train.append(spk)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, spk_train, spk_test\n",
    "\n",
    "\n",
    "def make_scale_fn(X_train):\n",
    "    # Standardize on train data\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(np.concatenate(X_train))\n",
    "    print(\"Normalization will be performed using mean: {}\".format(scaler.mean_))\n",
    "    print(\"Normalization will be performed using std: {}\".format(scaler.scale_))\n",
    "    def scale(X):\n",
    "        scaled = []\n",
    "\n",
    "        for frames in X:\n",
    "            scaled.append(scaler.transform(frames))\n",
    "        return scaled\n",
    "    return scale\n",
    "\n",
    "\n",
    "def parser(directory, n_mfcc=6):\n",
    "    wavs, Fs, ids, y, speakers = parse_free_digits(directory)\n",
    "    frames = extract_features(wavs, n_mfcc=n_mfcc, Fs=Fs)\n",
    "    make_scale_fn(frames)\n",
    "#     print(len(frames))\n",
    "#     print(len(ids))\n",
    "#     print(len(y))\n",
    "#     print(len(speakers))\n",
    "    X_train, X_test, y_train, y_test, spk_train, spk_test = split_free_digits(\n",
    "        frames, ids, speakers, y\n",
    "    )\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, spk_train, spk_test\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test, spk_train, spk_test = parser(\"recordings\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, stratify=y_train, test_size=0.20) \n",
    "#stratify to make sure we have the same analogy\n",
    "print(len(X_tr[0][1]))\n",
    "print(len(X_val))\n",
    "\n",
    "#10\n",
    "#necessary for model training/testing cause shapes don't fit otherwise\n",
    "digits_tr = [0,0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "for i in range(len(X_tr)):\n",
    "    if (type(digits_tr[y_tr[i]]) == int): \n",
    "        digits_tr[y_tr[i]] = X_tr[i]  #all instances of the same digit\n",
    "    else:\n",
    "        digits_tr[y_tr[i]] = np.concatenate((digits_tr[y_tr[i]], X_tr[i]), axis=0)\n",
    "\n",
    "        \n",
    "digits_val=[0,0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "for i in range(len(X_val)):\n",
    "    if (type(digits_val[y_val[i]]) == int): \n",
    "        digits_val[y_val[i]] = X_val[i]\n",
    "    else:\n",
    "        digits_val[y_val[i]] = np.concatenate((digits_val[y_val[i]], X_val[i]), axis=0)\n",
    "        \n",
    "        \n",
    "digits_test=[0,0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    if (type(digits_test[y_test[i]]) == int): \n",
    "        digits_test[y_test[i]] = X_test[i]\n",
    "    else:\n",
    "        digits_test[y_test[i]] = np.concatenate((digits_test[y_test[i]], X_test[i]), axis=0)\n",
    "        \n",
    "        \n",
    "        \n",
    "#its dimension would be num_sequences x seq_length x feature_dimension  ?      \n",
    "digits_train_3 = [[], [], [], [], \n",
    "                   [], [], [], [], \n",
    "                   [], []]\n",
    "\n",
    "for i in range(len(X_tr)):\n",
    "    digits_train_3[y_tr[i]].append(np.array(X_tr[i]))\n",
    "print(len(digits_train_3))\n",
    "print(len(X_train))\n",
    "\n",
    "\n",
    "\n",
    "digits_val_3 = [[], [], [], [], \n",
    "                [], [], [], [], \n",
    "                [], []]\n",
    "\n",
    "for i in range(len(X_val)):\n",
    "    digits_val_3[y_val[i]].append(np.array(X_val[i]))\n",
    "    \n",
    "    \n",
    "digits_test_3 = [[], [], [], [], \n",
    "                [], [], [], [], \n",
    "                [], []]\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    digits_test_3[y_test[i]].append(np.array(X_test[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "63a11c89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total wavs: 3000. Fs = 8000 Hz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Extracting mfcc features...: 100%|████████████████████████████████████████████████| 3000/3000 [00:08<00:00, 336.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature extraction completed with 6 mfccs per frame\n",
      "Normalization will be performed using mean: [-517.71365072   62.17300245   19.0018117     9.6444396   -19.211481\n",
      "  -10.88157066]\n",
      "Normalization will be performed using std: [152.11675796  52.36822505  36.82452714  29.71210521  24.82190463\n",
      "  23.43786656]\n",
      "Splitting in train test split using the default dataset split\n",
      "6\n",
      "540\n",
      "10\n",
      "2700\n"
     ]
    }
   ],
   "source": [
    "#9\n",
    "\n",
    "\n",
    "import os\n",
    "from glob import glob\n",
    "\n",
    "import librosa\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def parse_free_digits(directory):\n",
    "    # Parse relevant dataset info\n",
    "    files = glob(os.path.join(directory, \"*.wav\"))\n",
    "    \n",
    "    fnames = [f.split(\"\\\\\")[1].split(\".\")[0].split(\"_\") for f in files]\n",
    "    \n",
    "    \n",
    "    ids = [f[2] for f in fnames]\n",
    "    y = [int(f[0]) for f in fnames]\n",
    "    speakers = [f[1] for f in fnames]\n",
    "    _, Fs = librosa.core.load(files[0], sr=None)\n",
    "\n",
    "    def read_wav(f):\n",
    "        wav, _ = librosa.core.load(f, sr=None)\n",
    "\n",
    "        return wav\n",
    "\n",
    "    # Read all wavs\n",
    "    wavs = [read_wav(f) for f in files]\n",
    "\n",
    "    # Print dataset info\n",
    "    print(\"Total wavs: {}. Fs = {} Hz\".format(len(wavs), Fs))\n",
    "    #print(speakers)\n",
    "    return wavs, Fs, ids, y, speakers\n",
    "\n",
    "\n",
    "def extract_features(wavs, n_mfcc=6, Fs=8000):\n",
    "    # Extract MFCCs for all wavs\n",
    "    #print(len(wavs))\n",
    "    window = 30 * Fs // 1000\n",
    "    step = window // 2\n",
    "    frames = [\n",
    "        librosa.feature.mfcc(\n",
    "            wav, Fs, n_fft=window, hop_length=window - step, n_mfcc=n_mfcc\n",
    "        ).T\n",
    "\n",
    "        for wav in tqdm(wavs, desc=\"Extracting mfcc features...\")\n",
    "    ]\n",
    "\n",
    "    print(\"Feature extraction completed with {} mfccs per frame\".format(n_mfcc))\n",
    "     \n",
    "    return frames\n",
    "\n",
    "\n",
    "def split_free_digits(frames, ids, speakers, labels):\n",
    "    print(\"Splitting in train test split using the default dataset split\")\n",
    "    # Split to train-test\n",
    "    X_train, y_train, spk_train = [], [], []\n",
    "    X_test, y_test, spk_test = [], [], []\n",
    "    test_indices = [\"0\", \"1\", \"2\", \"3\", \"4\"]\n",
    "    \n",
    "    for idx, frame, label, spk in zip(ids, frames, labels, speakers):\n",
    "        if str(idx) in test_indices:\n",
    "            X_test.append(frame)\n",
    "            y_test.append(label)\n",
    "            spk_test.append(spk)\n",
    "        else:\n",
    "            X_train.append(frame)\n",
    "            y_train.append(label)\n",
    "            spk_train.append(spk)\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, spk_train, spk_test\n",
    "\n",
    "\n",
    "def make_scale_fn(X_train):\n",
    "    # Standardize on train data\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(np.concatenate(X_train))\n",
    "    print(\"Normalization will be performed using mean: {}\".format(scaler.mean_))\n",
    "    print(\"Normalization will be performed using std: {}\".format(scaler.scale_))\n",
    "    def scale(X):\n",
    "        scaled = []\n",
    "\n",
    "        for frames in X:\n",
    "            scaled.append(scaler.transform(frames))\n",
    "        return scaled\n",
    "    return scale\n",
    "\n",
    "\n",
    "def parser(directory, n_mfcc=6):\n",
    "    wavs, Fs, ids, y, speakers = parse_free_digits(directory)\n",
    "    frames = extract_features(wavs, n_mfcc=n_mfcc, Fs=Fs)\n",
    "    make_scale_fn(frames)\n",
    "#     print(len(frames))\n",
    "#     print(len(ids))\n",
    "#     print(len(y))\n",
    "#     print(len(speakers))\n",
    "    X_train, X_test, y_train, y_test, spk_train, spk_test = split_free_digits(\n",
    "        frames, ids, speakers, y\n",
    "    )\n",
    "\n",
    "    return X_train, X_test, y_train, y_test, spk_train, spk_test\n",
    "\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test, spk_train, spk_test = parser(\"recordings\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, stratify=y_train, test_size=0.20) \n",
    "#stratify to make sure we have the same analogy\n",
    "print(len(X_tr[0][1]))\n",
    "print(len(X_val))\n",
    "\n",
    "#10\n",
    "#necessary for model training/testing cause shapes don't fit otherwise\n",
    "digits_tr = [0,0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "for i in range(len(X_tr)):\n",
    "    if (type(digits_tr[y_tr[i]]) == int): \n",
    "        digits_tr[y_tr[i]] = X_tr[i]  #all instances of the same digit\n",
    "    else:\n",
    "        digits_tr[y_tr[i]] = np.concatenate((digits_tr[y_tr[i]], X_tr[i]), axis=0)\n",
    "\n",
    "        \n",
    "digits_val=[0,0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "for i in range(len(X_val)):\n",
    "    if (type(digits_val[y_val[i]]) == int): \n",
    "        digits_val[y_val[i]] = X_val[i]\n",
    "    else:\n",
    "        digits_val[y_val[i]] = np.concatenate((digits_val[y_val[i]], X_val[i]), axis=0)\n",
    "        \n",
    "        \n",
    "digits_test=[0,0,0,0,0,0,0,0,0,0]\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    if (type(digits_test[y_test[i]]) == int): \n",
    "        digits_test[y_test[i]] = X_test[i]\n",
    "    else:\n",
    "        digits_test[y_test[i]] = np.concatenate((digits_test[y_test[i]], X_test[i]), axis=0)\n",
    "        \n",
    "        \n",
    "        \n",
    "#its dimension would be num_sequences x seq_length x feature_dimension  ?      \n",
    "digits_train_3 = [[], [], [], [], \n",
    "                   [], [], [], [], \n",
    "                   [], []]\n",
    "\n",
    "for i in range(len(X_tr)):\n",
    "    digits_train_3[y_tr[i]].append(np.array(X_tr[i]))\n",
    "print(len(digits_train_3))\n",
    "print(len(X_train))\n",
    "\n",
    "\n",
    "\n",
    "digits_val_3 = [[], [], [], [], \n",
    "                [], [], [], [], \n",
    "                [], []]\n",
    "\n",
    "for i in range(len(X_val)):\n",
    "    digits_val_3[y_val[i]].append(np.array(X_val[i]))\n",
    "    \n",
    "    \n",
    "digits_test_3 = [[], [], [], [], \n",
    "                [], [], [], [], \n",
    "                [], []]\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    digits_test_3[y_test[i]].append(np.array(X_test[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4474b7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#14\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class FrameLevelDataset(Dataset):\n",
    "    nn.Dropout(0.25)\n",
    "    def __init__(self, feats, labels):\n",
    "        \"\"\"\n",
    "            feats: Python list of numpy arrays that contain the sequence features.\n",
    "                   Each element of this list is a numpy array of shape seq_length x feature_dimension\n",
    "            labels: Python list that contains the label for each sequence (each label must be an integer)\n",
    "        \"\"\"\n",
    "        max_seq_len = 0\n",
    "        #print(type(feats))\n",
    "        #print(len(feats))\n",
    "        lengths=np.zeros(len(feats))\n",
    "        #finding the max seq len\n",
    "        for i in range(len(feats)):\n",
    "            if (len(feats[i])>max_seq_len):\n",
    "                max_seq_len = feats[i].shape[0]\n",
    "            lengths[i] = feats[i].shape[0]\n",
    "        print(lengths)\n",
    "        \n",
    "        self.lengths = lengths\n",
    "        self.maxseqlen = max_seq_len\n",
    "        print(max_seq_len)\n",
    "        print(\"len feats is\" + str(len(feats)))\n",
    "        self.feats = self.zero_pad_and_stack(feats)\n",
    "        if isinstance(labels, (list, tuple)):\n",
    "            self.labels = np.array(labels).astype('int64')\n",
    "\n",
    "    def zero_pad_and_stack(self, x):\n",
    "        \"\"\"\n",
    "            This function performs zero padding on a list of features and forms them into a numpy 3D array\n",
    "            returns\n",
    "                padded: a 3D numpy array of shape num_sequences x max_sequence_length x feature_dimension\n",
    "        \"\"\"\n",
    "        #print(x[0])\n",
    "        padded = np.zeros((len(x),self.maxseqlen,6))\n",
    "        for i in range(len(x)):\n",
    "            for j in range(self.maxseqlen):\n",
    "                if(j>=x[i].shape[0]):\n",
    "                    for k in range(6):\n",
    "                        padded[i][j][k] = 0\n",
    "                else:\n",
    "                    #print(x[i][j])\n",
    "                    padded[i][j] = x[i][j]\n",
    "        \n",
    "        # --------------- Insert your code here ---------------- #\n",
    "        #print((padded))\n",
    "        return padded\n",
    "\n",
    "    def __getitem__(self, item):\n",
    "        return self.feats[item], self.labels[item], self.lengths[item]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.feats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6684298f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.autograd import Variable \n",
    "class BasicLSTM(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_size, output_dim, num_layers, bidirectional=False):\n",
    "        super(BasicLSTM, self).__init__()\n",
    "        self.bidirectional = bidirectional\n",
    "        self.feature_size = hidden_size* 2 if self.bidirectional else hidden_size\n",
    "\n",
    "        # --------------- Insert your code here ---------------- #\n",
    "        \n",
    "        self.num_layers = num_layers #number of layers\n",
    "        self.input_size = input_size #input size\n",
    "        self.hidden_size = hidden_size #hidden state\n",
    "        \n",
    "        self.lstm = nn.LSTM(input_size=input_dim, hidden_size=hidden_size,\n",
    "                          num_layers=num_layers, batch_first=True) #lstm\n",
    "        self.fc =  nn.Linear(hidden_size, output_dim) #fully connected final layers\n",
    "    \n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x, lengths):\n",
    "        \"\"\" \n",
    "            x : 3D numpy array of dimension N x L x D\n",
    "                N: batch index\n",
    "                L: sequence index\n",
    "                D: feature index\n",
    "            lengths: N x 1\n",
    "         \"\"\"\n",
    "        \n",
    "        # --------------- Insert your code here ---------------- #\n",
    "        \n",
    "        # You must have all of the outputs of the LSTM, but you need only the last one (that does not exceed the sequence length)\n",
    "        # To get it use the last_timestep method\n",
    "        # Then pass it through the remaining network\n",
    "        x = x.float()\n",
    "        bi = 2 if self.bidirectional else 1\n",
    "        h_0 = torch.zeros(self.num_layers*bi, x.size(0), self.hidden_size) #hidden state\n",
    "        c_0 = torch.zeros(self.num_layers*bi, x.size(0), self.hidden_size) #internal state\n",
    "        # Propagate input through LSTM\n",
    "        output, _= self.lstm(x, (h_0, c_0)) #lstm with input, hidden, and internal state\n",
    "       # hn = hn.view(-1, self.hidden_size) #reshaping the data for Dense layer next\n",
    "        #output = self.relu(hn)\n",
    "        output = self.fc(output) #final output\n",
    "        #print(type(output))\n",
    "        last_outputs = self.last_timestep(output,lengths,bidirectional=self.bidirectional)\n",
    "        #print(last_outputs)\n",
    "        return last_outputs\n",
    "\n",
    "    def last_timestep(self, outputs, lengths, bidirectional=False):\n",
    "        \"\"\"\n",
    "            Returns the last output of the LSTM taking into account the zero padding\n",
    "        \"\"\"\n",
    "        if bidirectional:\n",
    "            forward, backward = self.split_directions(outputs)\n",
    "            last_forward = self.last_by_index(forward, lengths)\n",
    "            last_backward = backward[:, 0, :]\n",
    "            # Concatenate and return - maybe add more functionalities like average\n",
    "            return torch.cat((last_forward, last_backward), dim=-1)\n",
    "\n",
    "        else:\n",
    "            return self.last_by_index(outputs, lengths)\n",
    "\n",
    "    @staticmethod\n",
    "    def split_directions(outputs):\n",
    "        direction_size = int(outputs.size(-1) / 2)\n",
    "        forward = outputs[:, :, :direction_size]\n",
    "        backward = outputs[:, :, direction_size:]\n",
    "        return forward, backward\n",
    "\n",
    "    @staticmethod\n",
    "    def last_by_index(outputs, lengths):\n",
    "        # Index of the last output for each sequence.\n",
    "        idx = (lengths - 1).view(-1, 1).expand(outputs.size(0),\n",
    "                                               outputs.size(2)).unsqueeze(1)\n",
    "        idx = idx.type(torch.int64)\n",
    "        #print(type(idx))\n",
    "        return outputs.gather(1, idx).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bec3649f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_epochs = 60 #1000 epochs\n",
    "learning_rate = 0.001 #0.001 lr\n",
    "input_size = 6 #number of features\n",
    "hidden_size = 150 #number of features in hidden state\n",
    "num_layers = 2 #number of stacked lstm layers\n",
    "bidirectional = False\n",
    "num_classes = 10 #number of output classes \n",
    "batch_size = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f6d224ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[36. 39. 36. ... 26. 27. 37.]\n",
      "147\n",
      "len feats is2160\n",
      "[20. 40. 45. 42. 37. 43. 36. 36. 40. 37. 43. 46. 49. 38. 34. 30. 32. 24.\n",
      " 37. 33. 27. 24. 23. 23. 28. 26. 23. 24. 24. 22. 38. 34. 39. 36. 36. 35.\n",
      " 36. 32. 34. 36. 26. 27. 28. 54. 26. 25. 20. 18. 20. 20. 16. 16. 13. 17.\n",
      " 15. 28. 15. 23. 21. 21. 23. 38. 27. 27. 26. 34. 37. 30. 34. 32. 25. 28.\n",
      " 29. 29. 29. 24. 20. 25. 16. 23. 17. 16. 36. 14. 18. 19. 21. 16. 19. 20.\n",
      " 34. 34. 33. 36. 30. 33. 32. 34. 35. 30. 42. 41. 39. 31. 36. 23. 22. 18.\n",
      " 16. 24. 17. 19. 19. 16. 15. 27. 21. 17. 27. 27. 30. 36. 33. 32. 29. 31.\n",
      " 28. 28. 28. 29. 29. 28. 42. 30. 37. 21. 24. 21. 22. 22. 19. 17. 16. 17.\n",
      " 20. 28. 27. 23. 19. 24. 38. 39. 33. 34. 32. 29. 28. 31. 27. 36. 41. 77.\n",
      " 39. 36. 34. 23. 26. 21. 25. 24. 21. 20. 18. 19. 19. 21. 28. 26. 28. 34.\n",
      " 35. 32. 38. 40. 37. 56. 43. 43. 58. 42. 33. 42. 33. 59. 33. 15. 16. 18.\n",
      " 27. 32. 33. 33. 34. 33. 32. 23. 11. 16. 10. 13. 43. 40. 44. 39. 42. 29.\n",
      " 32. 26. 29. 28. 45. 31. 32. 38. 39. 25. 31. 30. 25. 30. 29. 25. 17. 20.\n",
      " 29. 30. 27. 29. 29. 25. 36. 35. 37. 34. 34. 24. 27. 26. 26. 28. 77. 23.\n",
      " 55. 47. 46. 16. 16. 17. 17. 18. 25. 22. 25. 20. 24. 22. 24. 19. 22. 22.\n",
      " 35. 34. 34. 23. 33. 41. 38. 39. 36. 39. 35. 38. 32. 31. 32. 28. 33. 30.\n",
      " 30. 24. 26. 20. 19. 30. 30. 24. 26. 27. 37. 29.]\n",
      "77\n",
      "len feats is300\n",
      "[ 25.  29.  31.  27.  19.  34.  28.  26.  43.  22.  17.  31.  25.  28.\n",
      "  41.  20.  28.  30.  28.  28.  24.  35.  14.  28.  40.  58.  17.  22.\n",
      "  38.  28.  30.  35.  34.  37.  42.  34.  18.  41.  24.  17.  36.  24.\n",
      "  58.  30.  24.  26.  21.  36. 153.  21.  29.  34.  34.  18.  41.  24.\n",
      "  24.  64.  21.  29.  29.  33.  35.  25.  22.  18.  31.  24.  27.  18.\n",
      "  20.  29.  28.  32.  32.  23.  30.  39.  16.  27.  41.  29.  24.  20.\n",
      "  21.  19.  38.  29.  23.  18.  20.  40.  31.  38.  48.  27.  40.  50.\n",
      "  38.  26.  29.  39.  87.  19.  16.  27.  25.  16.  29.  21.  37.  30.\n",
      "  25.  30.  26.  28.  35.  21.  26.  33.  23.  34.  30.  25.  25.  42.\n",
      "  18.  25.  17.  27.  28.  42.  21.  31.  22.  22.  27.  26.  23.  28.\n",
      "  25.  28.  34.  35.  43.  35.  23.  25.  25.  28.  44.  23.  44.  34.\n",
      "  21.  33.  27.  39.  40.  31.  26.  42.  41.  27.  63.  18.  29.  32.\n",
      "  29.  21.  32.  32.  42.  24.  30.  41.  44.  31.  40.  25.  27.  47.\n",
      "  28.  36.  31.  34.  27.  19.  28.  33.  37.  24.  18.  19.  27.  37.\n",
      "  27.  21.  32.  30.  35.  20.  28.  28.  26.  19.  19.  41.  28.  26.\n",
      "  23.  18.  32.  32.  29.  31.  36.  26.  41.  27.  30.  28.  31.  25.\n",
      "  25.  22.  30.  28.  20.  26.  19.  33.  24.  31.  16.  32.  27.  31.\n",
      "  19.  31.  32.  36.  33.  24.  21.  19.  25.  35.  42.  39.  28.  18.\n",
      "  29.  32.  16.  18.  36.  22.  19.  26.  43.  21.  24.  28.  27.  20.\n",
      "  32.  34.  29.  24.  27.  44.  17.  23.  29.  36.  25.  33.  26.  22.\n",
      "  33.  19.  18.  41.  27.  20.  40.  24.  24.  26.  27.  24.  28.  38.\n",
      "  24.  48.  38.  22.  34.  23.  24.  27.  37.  26.  30.  23.  67.  30.\n",
      "  37.  37.  19.  24.  45.  21.  31.  22.  39.  31.  42.  16.  29.  59.\n",
      "  35.  23.  37.  34.  54.  33.  34.  19.  36.  39.  23.  16.  22.  26.\n",
      "  27.  32.  37.  31.  30.  13.  26.  29.  21.  29.  34.  23.  42.  43.\n",
      "  41.  44.  47.  29.  29.  32.  39.  33.  30.  30.  21.  51.  22.  48.\n",
      "  28.  16.  14.  23.  32.  21.  23.  28.  28.  34.  42.  43.  28.  28.\n",
      "  46.  14.  20.  25.  23.  31.  28.  45.  27.  33.  27.  30.  19.  28.\n",
      "  24.  24.  26.  31.  39.  26.  19.  40.  30.  23.  28.  59.  30.  15.\n",
      "  43.  24.  24.  16.  28.  22.  26.  19.  18.  29.  18.  54.  23.  31.\n",
      "  28.  28.  37.  27.  17.  37.  27.  22.  37.  24.  30.  15.  29.  31.\n",
      "  26.  25.  38.  30.  23.  35.  34.  24.  38.  41.  31.  20.  52.  17.\n",
      "  34.  17.  20.  45.  70.  39.  28.  34.  27.  23.  25.  42.  41.  36.\n",
      "  26.  17.  29.  35.  26.  34.  57.  36.  22.  28.  35.  18.  26.  28.\n",
      "  40.  25.  27.  17.  50.  29.  14.  28.  23.  32.  20.  25.  33.  29.\n",
      "  38.  25.  35.  29.  23.  29.  33.  23.  37.  21.  29.  19.  46.  23.\n",
      "  21.  27.  43.  25.  28.  30.  33.  31.  30.  26.  20.  75.  23.  26.\n",
      "  18.  29.  28.  33.  47.  29.  34.  37.  29.  24.  26.  27.  34.  20.\n",
      "  19.  27.  32.  19.  22.  31.  62.  26.]\n",
      "153\n",
      "len feats is540\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x0000029223FEEB38>\n"
     ]
    }
   ],
   "source": [
    "train_set = FrameLevelDataset(X_tr,y_tr)\n",
    "test_set = FrameLevelDataset(X_test,y_test)\n",
    "val_set = FrameLevelDataset(X_val,y_val )\n",
    "model = BasicLSTM(input_size, hidden_size, num_classes, num_layers, bidirectional=bidirectional) #our lstm class \n",
    "model = model.float()\n",
    "train_dataloader = torch.utils.data.DataLoader(train_set, batch_size=batch_size, shuffle=True)\n",
    "print(train_dataloader)\n",
    "test_dataloader = torch.utils.data.DataLoader(test_set, batch_size=batch_size, shuffle=False)\n",
    "val_dataloader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "654f985a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\30694\\anaconda3\\envs\\lab2\\lib\\site-packages\\ipykernel_launcher.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "C:\\Users\\30694\\anaconda3\\envs\\lab2\\lib\\site-packages\\ipykernel_launcher.py:42: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0/60] train_loss: 1.48617 valid_loss: 0.76354\n",
      "[ 1/60] train_loss: 0.61032 valid_loss: 0.39333\n",
      "[ 2/60] train_loss: 0.38444 valid_loss: 0.39203\n",
      "[ 3/60] train_loss: 0.26996 valid_loss: 0.39014\n",
      "[ 4/60] train_loss: 0.26279 valid_loss: 0.23964\n",
      "[ 5/60] train_loss: 0.16786 valid_loss: 0.26269\n",
      "[ 6/60] train_loss: 0.14451 valid_loss: 0.15258\n",
      "[ 7/60] train_loss: 0.08040 valid_loss: 0.14860\n",
      "[ 8/60] train_loss: 0.13722 valid_loss: 0.29279\n",
      "[ 9/60] train_loss: 0.16148 valid_loss: 0.19264\n",
      "[10/60] train_loss: 0.12483 valid_loss: 0.12157\n",
      "[11/60] train_loss: 0.11638 valid_loss: 0.20381\n",
      "[12/60] train_loss: 0.11001 valid_loss: 0.14205\n",
      "[13/60] train_loss: 0.08599 valid_loss: 0.19162\n",
      "[14/60] train_loss: 0.12832 valid_loss: 0.10436\n",
      "[15/60] train_loss: 0.09410 valid_loss: 0.12878\n",
      "[16/60] train_loss: 0.07434 valid_loss: 0.09909\n",
      "[17/60] train_loss: 0.13212 valid_loss: 0.10536\n",
      "[18/60] train_loss: 0.07456 valid_loss: 0.12127\n",
      "[19/60] train_loss: 0.07010 valid_loss: 0.11112\n",
      "[20/60] train_loss: 0.08459 valid_loss: 0.21081\n",
      "[21/60] train_loss: 0.11820 valid_loss: 0.10912\n",
      "[22/60] train_loss: 0.03402 valid_loss: 0.12038\n",
      "[23/60] train_loss: 0.04780 valid_loss: 0.09703\n",
      "[24/60] train_loss: 0.05488 valid_loss: 0.09588\n",
      "[25/60] train_loss: 0.02024 valid_loss: 0.09089\n",
      "[26/60] train_loss: 0.05207 valid_loss: 0.07977\n",
      "[27/60] train_loss: 0.04570 valid_loss: 0.09461\n",
      "[28/60] train_loss: 0.05066 valid_loss: 0.08304\n",
      "[29/60] train_loss: 0.03757 valid_loss: 0.07416\n",
      "[30/60] train_loss: 0.02739 valid_loss: 0.08146\n",
      "[31/60] train_loss: 0.02369 valid_loss: 0.16608\n",
      "[32/60] train_loss: 0.03320 valid_loss: 0.13215\n",
      "[33/60] train_loss: 0.02101 valid_loss: 0.05957\n",
      "[34/60] train_loss: 0.04198 valid_loss: 0.13372\n",
      "[35/60] train_loss: 0.06717 valid_loss: 0.12133\n",
      "[36/60] train_loss: 0.09225 valid_loss: 0.13314\n",
      "[37/60] train_loss: 0.04726 valid_loss: 0.06516\n",
      "[38/60] train_loss: 0.01279 valid_loss: 0.07948\n",
      "[39/60] train_loss: 0.01489 valid_loss: 0.05990\n",
      "[40/60] train_loss: 0.03240 valid_loss: 0.07765\n",
      "[41/60] train_loss: 0.07473 valid_loss: 0.13307\n",
      "[42/60] train_loss: 0.05002 valid_loss: 0.04765\n",
      "[43/60] train_loss: 0.02746 valid_loss: 0.09341\n",
      "[44/60] train_loss: 0.02445 valid_loss: 0.10156\n",
      "[45/60] train_loss: 0.04967 valid_loss: 0.08432\n",
      "[46/60] train_loss: 0.09606 valid_loss: 0.12458\n",
      "[47/60] train_loss: 0.04002 valid_loss: 0.08082\n",
      "[48/60] train_loss: 0.01168 valid_loss: 0.08150\n",
      "[49/60] train_loss: 0.00686 valid_loss: 0.09100\n",
      "[50/60] train_loss: 0.01199 valid_loss: 0.32544\n",
      "[51/60] train_loss: 0.25968 valid_loss: 0.10928\n",
      "[52/60] train_loss: 0.05304 valid_loss: 0.11354\n",
      "[53/60] train_loss: 0.04680 valid_loss: 0.07572\n",
      "[54/60] train_loss: 0.01830 valid_loss: 0.09039\n",
      "[55/60] train_loss: 0.00707 valid_loss: 0.08555\n",
      "[56/60] train_loss: 0.00477 valid_loss: 0.08399\n",
      "[57/60] train_loss: 0.00384 valid_loss: 0.08246\n",
      "[58/60] train_loss: 0.00364 valid_loss: 0.08269\n",
      "[59/60] train_loss: 0.00302 valid_loss: 0.08414\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.CrossEntropyLoss()   # mean-squared error for regression\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate, weight_decay=1e-5)\n",
    "\n",
    "# to track the training loss as the model trains\n",
    "train_losses = []\n",
    "    # to track the validation loss as the model trains\n",
    "valid_losses = []\n",
    "    # to track the average training loss per epoch as the model trains\n",
    "avg_train_losses = []\n",
    "    # to track the average validation loss per epoch as the model trains\n",
    "avg_valid_losses = [] \n",
    "    \n",
    "    # initialize the early_stopping object\n",
    "#early_stopping = EarlyStopping(patience=patience, verbose=True)\n",
    "    \n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "        ###################\n",
    "        # train the model #\n",
    "        ###################\n",
    "    model.train() # prep model for training\n",
    "    for batch_idx, (feats,labels,lengths) in enumerate(train_dataloader):\n",
    "        feats = feats.float()\n",
    "        labels = torch.tensor(labels, dtype=torch.long)\n",
    "        outputs = model(feats,lengths) #forward pass\n",
    "        #caluclate the gradient, manually setting to 0\n",
    "        optimizer.zero_grad()\n",
    "                        #print(net_out.size())\n",
    "        loss = criterion(outputs,labels)\n",
    "        #print(loss.size())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "            # record training loss\n",
    "        train_losses.append(loss.item())\n",
    "\n",
    "        ######################    \n",
    "        # validate the model #\n",
    "        ######################\n",
    "        model.eval() # prep model for evaluation\n",
    "    for batch_idx, (feats,labels,lengths) in enumerate(val_dataloader):\n",
    "        feats = feats.float()\n",
    "        labels = torch.tensor(labels, dtype=torch.long)\n",
    "        outputs = model(feats,lengths) #forward pass\n",
    "        #caluclate the gradient, manually setting to 0\n",
    "       \n",
    "                        #print(net_out.size())\n",
    "        loss = criterion(outputs,labels)\n",
    "        #print(loss.size())\n",
    "        \n",
    "        valid_losses.append(loss.item())\n",
    "\n",
    "        # print training/validation statistics \n",
    "        # calculate average loss over an epoch\n",
    "    train_loss = np.average(train_losses)\n",
    "    valid_loss = np.average(valid_losses)\n",
    "    avg_train_losses.append(train_loss)\n",
    "    avg_valid_losses.append(valid_loss)\n",
    "        \n",
    "    epoch_len = len(str(num_epochs))\n",
    "        \n",
    "    print_msg = (f'[{epoch:>{epoch_len}}/{num_epochs:>{epoch_len}}] ' +\n",
    "                     f'train_loss: {train_loss:.5f} ' +\n",
    "                     f'valid_loss: {valid_loss:.5f}')\n",
    "        \n",
    "    print(print_msg)\n",
    "        \n",
    "        # clear lists to track next epoch\n",
    "    train_losses = []\n",
    "    valid_losses = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f44fb9bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 25.  29.  31.  27.  19.  34.  28.  26.  43.  22.  17.  31.  25.  28.\n",
      "  41.  20.  28.  30.  28.  28.  24.  35.  14.  28.  40.  58.  17.  22.\n",
      "  38.  28.  30.  35.  34.  37.  42.  34.  18.  41.  24.  17.  36.  24.\n",
      "  58.  30.  24.  26.  21.  36. 153.  21.  29.  34.  34.  18.  41.  24.\n",
      "  24.  64.  21.  29.  29.  33.  35.  25.  22.  18.  31.  24.  27.  18.\n",
      "  20.  29.  28.  32.  32.  23.  30.  39.  16.  27.  41.  29.  24.  20.\n",
      "  21.  19.  38.  29.  23.  18.  20.  40.  31.  38.  48.  27.  40.  50.\n",
      "  38.  26.  29.  39.  87.  19.  16.  27.  25.  16.  29.  21.  37.  30.\n",
      "  25.  30.  26.  28.  35.  21.  26.  33.  23.  34.  30.  25.  25.  42.\n",
      "  18.  25.  17.  27.  28.  42.  21.  31.  22.  22.  27.  26.  23.  28.\n",
      "  25.  28.  34.  35.  43.  35.  23.  25.  25.  28.  44.  23.  44.  34.\n",
      "  21.  33.  27.  39.  40.  31.  26.  42.  41.  27.  63.  18.  29.  32.\n",
      "  29.  21.  32.  32.  42.  24.  30.  41.  44.  31.  40.  25.  27.  47.\n",
      "  28.  36.  31.  34.  27.  19.  28.  33.  37.  24.  18.  19.  27.  37.\n",
      "  27.  21.  32.  30.  35.  20.  28.  28.  26.  19.  19.  41.  28.  26.\n",
      "  23.  18.  32.  32.  29.  31.  36.  26.  41.  27.  30.  28.  31.  25.\n",
      "  25.  22.  30.  28.  20.  26.  19.  33.  24.  31.  16.  32.  27.  31.\n",
      "  19.  31.  32.  36.  33.  24.  21.  19.  25.  35.  42.  39.  28.  18.\n",
      "  29.  32.  16.  18.  36.  22.  19.  26.  43.  21.  24.  28.  27.  20.\n",
      "  32.  34.  29.  24.  27.  44.  17.  23.  29.  36.  25.  33.  26.  22.\n",
      "  33.  19.  18.  41.  27.  20.  40.  24.  24.  26.  27.  24.  28.  38.\n",
      "  24.  48.  38.  22.  34.  23.  24.  27.  37.  26.  30.  23.  67.  30.\n",
      "  37.  37.  19.  24.  45.  21.  31.  22.  39.  31.  42.  16.  29.  59.\n",
      "  35.  23.  37.  34.  54.  33.  34.  19.  36.  39.  23.  16.  22.  26.\n",
      "  27.  32.  37.  31.  30.  13.  26.  29.  21.  29.  34.  23.  42.  43.\n",
      "  41.  44.  47.  29.  29.  32.  39.  33.  30.  30.  21.  51.  22.  48.\n",
      "  28.  16.  14.  23.  32.  21.  23.  28.  28.  34.  42.  43.  28.  28.\n",
      "  46.  14.  20.  25.  23.  31.  28.  45.  27.  33.  27.  30.  19.  28.\n",
      "  24.  24.  26.  31.  39.  26.  19.  40.  30.  23.  28.  59.  30.  15.\n",
      "  43.  24.  24.  16.  28.  22.  26.  19.  18.  29.  18.  54.  23.  31.\n",
      "  28.  28.  37.  27.  17.  37.  27.  22.  37.  24.  30.  15.  29.  31.\n",
      "  26.  25.  38.  30.  23.  35.  34.  24.  38.  41.  31.  20.  52.  17.\n",
      "  34.  17.  20.  45.  70.  39.  28.  34.  27.  23.  25.  42.  41.  36.\n",
      "  26.  17.  29.  35.  26.  34.  57.  36.  22.  28.  35.  18.  26.  28.\n",
      "  40.  25.  27.  17.  50.  29.  14.  28.  23.  32.  20.  25.  33.  29.\n",
      "  38.  25.  35.  29.  23.  29.  33.  23.  37.  21.  29.  19.  46.  23.\n",
      "  21.  27.  43.  25.  28.  30.  33.  31.  30.  26.  20.  75.  23.  26.\n",
      "  18.  29.  28.  33.  47.  29.  34.  37.  29.  24.  26.  27.  34.  20.\n",
      "  19.  27.  32.  19.  22.  31.  62.  26.]\n",
      "153\n",
      "len feats is540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\30694\\anaconda3\\envs\\lab2\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0005, Accuracy: 19/540 (4%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0004, Accuracy: 37/540 (7%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 57/540 (11%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 77/540 (14%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 97/540 (18%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0008, Accuracy: 114/540 (21%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 134/540 (25%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 154/540 (29%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0001, Accuracy: 173/540 (32%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 193/540 (36%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 213/540 (39%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 233/540 (43%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 253/540 (47%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 273/540 (51%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 293/540 (54%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0001, Accuracy: 312/540 (58%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 332/540 (61%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0006, Accuracy: 351/540 (65%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 371/540 (69%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0004, Accuracy: 390/540 (72%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 410/540 (76%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 430/540 (80%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 450/540 (83%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 470/540 (87%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 490/540 (91%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0007, Accuracy: 508/540 (94%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0005, Accuracy: 526/540 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "val_set = FrameLevelDataset(X_val,y_val )\n",
    "val_dataloader = torch.utils.data.DataLoader(val_set, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "y_pred=np.array([])\n",
    "y_true=np.array([])\n",
    "for batch_idx, (feats,labels,lengths) in enumerate(val_dataloader):\n",
    "    feats = feats.float()\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    outputs = model(feats,lengths) #forward pass\n",
    "        #caluclate the gradient, manually setting to 0    \n",
    "    test_loss += criterion(outputs, labels).item()\n",
    "    pred = outputs.data.max(1)[1]  # get the index of the max log-probability\n",
    "    y_true=np.concatenate((y_true,labels.numpy()),axis=0)\n",
    "    y_pred=np.concatenate((y_pred,pred.numpy()),axis=0)\n",
    "    correct += pred.eq(labels).sum()\n",
    "\n",
    "\n",
    "    test_loss /= len(val_dataloader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            test_loss, correct, len(val_dataloader.dataset),\n",
    "            100. * correct / len(val_dataloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8af6bd06",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\30694\\anaconda3\\envs\\lab2\\lib\\site-packages\\ipykernel_launcher.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  import sys\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 20/300 (7%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0009, Accuracy: 39/300 (13%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 59/300 (20%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 79/300 (26%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 99/300 (33%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 119/300 (40%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0008, Accuracy: 138/300 (46%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 158/300 (53%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 178/300 (59%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0013, Accuracy: 197/300 (66%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 217/300 (72%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 237/300 (79%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0014, Accuracy: 255/300 (85%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0000, Accuracy: 275/300 (92%)\n",
      "\n",
      "\n",
      "Test set: Average loss: 0.0006, Accuracy: 293/300 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_loss = 0\n",
    "correct = 0\n",
    "y_pred=np.array([])\n",
    "y_true=np.array([])\n",
    "for batch_idx, (feats,labels,lengths) in enumerate(test_dataloader):\n",
    "    feats = feats.float()\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    outputs = model(feats,lengths) #forward pass\n",
    "        #caluclate the gradient, manually setting to 0    \n",
    "    test_loss += criterion(outputs, labels).item()\n",
    "    pred = outputs.data.max(1)[1]  # get the index of the max log-probability\n",
    "    y_true=np.concatenate((y_true,labels.numpy()),axis=0)\n",
    "    y_pred=np.concatenate((y_pred,pred.numpy()),axis=0)\n",
    "    correct += pred.eq(labels).sum()\n",
    "\n",
    "\n",
    "    test_loss /= len(test_dataloader.dataset)\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "            test_loss, correct, len(test_dataloader.dataset),\n",
    "            100. * correct / len(test_dataloader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d65771",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:lab2]",
   "language": "python",
   "name": "conda-env-lab2-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
